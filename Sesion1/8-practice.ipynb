{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8QtoXwvzeZQ"
      },
      "source": [
        "# NLP Basics Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNG-UgJDzeZX"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cam2149/icesi-nlp/blob/main/Sesion1/8-practice.ipynb)\n",
        "\n",
        "En este notebook vamos a poner en práctica algunos de los conceptos vistos en los notebooks anteriores, aplicado a un corpus específico:\n",
        "\n",
        "Con tantos tuits circulando a cada segundo, es difícil determinar si el sentimiento detrás de un tuit específico impactará la marca de una empresa o persona por ser viral (positivo), o si devastará las ganancias por su tono negativo. Capturar el sentimiento con palabras es importante en estos tiempos donde las decisiones y reacciones se crean y actualizan en segundos. Pero, ¿qué palabras conducen realmente a la descripción del sentimiento? En esta competencia, tendrás que identificar la parte del tuit (palabra o frase) que refleje el sentimiento.\n",
        "\n",
        "\"Mi perro ridículo es increíble.\" [sentimiento: positivo]\n",
        "\n",
        "Desarrollar habilidades en esta importante área con este amplio conjunto de datos de tuits. Perfecciona tu técnica para alcanzar el primer puesto en esta competencia. ¿Qué palabras en los tuits respaldan un sentimiento positivo, negativo o neutral? ¿Cómo puedes ayudar a determinarlo usando herramientas de aprendizaje automático?\n",
        "\n",
        "El conjunto de datos se titula \"Análisis de Sentimiento: Emoción en Tweets de Texto con Etiquetas de Sentimiento existentes\", utilizado aquí bajo la licencia Creative Commons Atribución 4.0 Internacional. El objetivo en este concurso es construir un modelo que pueda hacer lo mismo: analizar el sentimiento etiquetado de un tweet determinado y determinar qué palabra o frase lo respalda mejor.\n",
        "\n",
        "Descargo de responsabilidad: El conjunto de datos de este concurso contiene texto que puede considerarse profano, vulgar u ofensivo.\n",
        "\n",
        "## Referencias\n",
        "* [Extracción de sentimiento de tweets](https://www.kaggle.com/competitions/tweet-sentiment-extraction/overview)\n",
        "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt14wYva4ykp",
        "outputId": "b5431636-69a5-4d81-e38b-f218e6db234f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "X_TYq6mbzeZZ"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "import warnings\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from spacy.matcher import Matcher\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "installed_packages = [package.key for package in pkg_resources.working_set]\n",
        "IN_COLAB = 'google-colab' in installed_packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize SpaCy and VADER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Download and load the Kaggle dataset\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle competitions download -c tweet-sentiment-extraction\n",
        "!unzip -o tweet-sentiment-extraction.zip -d tweet_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alEuh6u_5dzF",
        "outputId": "0ffedc87-b235-4629-d356-38bc35706db6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'kaggle.json': No such file or directory\n",
            "tweet-sentiment-extraction.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  tweet-sentiment-extraction.zip\n",
            "  inflating: tweet_data/sample_submission.csv  \n",
            "  inflating: tweet_data/test.csv     \n",
            "  inflating: tweet_data/train.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XSOLJWUzzeZd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f188074-ecc2-4768-f6ca-a75cbbf7356a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-08-08 04:22:15--  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/Ohtar10/icesi-nlp/refs/heads/main/requirements.txt [following]\n",
            "--2025-08-08 04:22:15--  https://raw.githubusercontent.com/Ohtar10/icesi-nlp/refs/heads/main/requirements.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 349 [text/plain]\n",
            "Saving to: ‘requirements.txt’\n",
            "\n",
            "requirements.txt    100%[===================>]     349  --.-KB/s    in 0s      \n",
            "\n",
            "2025-08-08 04:22:15 (6.07 MB/s) - ‘requirements.txt’ saved [349/349]\n",
            "\n",
            "Collecting pandas==2.1.1 (from -r requirements.txt (line 1))\n",
            "  Downloading pandas-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting matplotlib==3.8.0 (from -r requirements.txt (line 2))\n",
            "  Downloading matplotlib-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting seaborn==0.12.2 (from -r requirements.txt (line 3))\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting scikit-learn==1.3.0 (from -r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting statsmodels==0.14.0 (from -r requirements.txt (line 5))\n",
            "  Downloading statsmodels-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting tqdm==4.66.1 (from -r requirements.txt (line 6))\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.2.0 (from -r requirements.txt (line 7))\n",
            "  Downloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting lightning==2.2.0.post0 (from -r requirements.txt (line 8))\n",
            "  Downloading lightning-2.2.0.post0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.16.2 (from -r requirements.txt (line 9))\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting bokeh==3.3.4 (from -r requirements.txt (line 10))\n",
            "  Downloading bokeh-3.3.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting transformers==4.41.2 (from transformers[torch]==4.41.2->-r requirements.txt (line 11))\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.19.1 (from -r requirements.txt (line 12))\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting torchinfo==1.8.0 (from -r requirements.txt (line 13))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting accelerate==0.30.1 (from -r requirements.txt (line 14))\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting evaluate==0.4.2 (from -r requirements.txt (line 15))\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting sentence-transformers==3.0.1 (from -r requirements.txt (line 16))\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gradio==4.36.1 (from -r requirements.txt (line 17))\n",
            "  Downloading gradio-4.36.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting ollama==0.2.1 (from -r requirements.txt (line 18))\n",
            "  Downloading ollama-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: spacy==3.8.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.8.7)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (1.4.8)\n",
            "Collecting numpy>=1.23.2 (from pandas==2.1.1->-r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (25.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from statsmodels==0.14.0->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning==2.2.0.post0->-r requirements.txt (line 8)) (6.0.2)\n",
            "Collecting fsspec (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning==2.2.0.post0->-r requirements.txt (line 8))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib==3.8.0->-r requirements.txt (line 2))\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning==2.2.0.post0->-r requirements.txt (line 8))\n",
            "  Downloading torchmetrics-1.8.1-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pytorch-lightning (from lightning==2.2.0.post0->-r requirements.txt (line 8))\n",
            "  Downloading pytorch_lightning-2.5.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.74.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (3.1.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.3.4->-r requirements.txt (line 10)) (6.4.2)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.11/dist-packages (from bokeh==3.3.4->-r requirements.txt (line 10)) (2025.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (0.34.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11))\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.1->-r requirements.txt (line 12))\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (0.70.16)\n",
            "Collecting fsspec (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (3.12.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.30.1->-r requirements.txt (line 14)) (5.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.6.1)\n",
            "Collecting gradio-client==1.0.1 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading gradio_client-1.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (6.5.2)\n",
            "Collecting markupsafe~=2.0 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (3.11.1)\n",
            "Collecting pillow>=6.2.0 (from matplotlib==3.8.0->-r requirements.txt (line 2))\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.12.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.10.0)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.16.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.35.0)\n",
            "Collecting httpx>=0.24.1 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (0.4.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk==3.9.1->-r requirements.txt (line 20)) (8.2.1)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.1->gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->-r requirements.txt (line 7)) (12.5.82)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2.0.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (1.1.5)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy==3.8.7->-r requirements.txt (line 19)) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (3.4.2)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy==3.8.7->-r requirements.txt (line 19)) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy==3.8.7->-r requirements.txt (line 19)) (0.1.5)\n",
            "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy==3.8.7->-r requirements.txt (line 19))\n",
            "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy==3.8.7->-r requirements.txt (line 19))\n",
            "  Downloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (7.3.0.post1)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi->gradio==4.36.1->-r requirements.txt (line 17)) (0.47.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.26.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy==3.8.7->-r requirements.txt (line 19)) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (0.1.2)\n",
            "Downloading pandas-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsmodels-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.0-cp311-cp311-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.2.0.post0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bokeh-3.3.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.36.1-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.2.1-py3-none-any.whl (9.7 kB)\n",
            "Downloading gradio_client-1.0.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.1-py3-none-any.whl (982 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytorch_lightning-2.5.2-py3-none-any.whl (825 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m825.4/825.4 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading blis-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.6/130.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: websockets, triton, tqdm, torchinfo, tomlkit, pyarrow-hotfix, pillow, packaging, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, markupsafe, fsspec, aiofiles, pandas, nvidia-cusolver-cu12, nvidia-cudnn-cu12, lightning-utilities, httpx, blis, torch, tokenizers, tensorboard, statsmodels, scikit-learn, ollama, matplotlib, gradio-client, bokeh, transformers, torchmetrics, thinc, seaborn, datasets, accelerate, sentence-transformers, pytorch-lightning, gradio, evaluate, lightning\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.3\n",
            "    Uninstalling tomlkit-0.13.3:\n",
            "      Successfully uninstalled tomlkit-0.13.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.4\n",
            "    Uninstalling tokenizers-0.21.4:\n",
            "      Successfully uninstalled tokenizers-0.21.4\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.5\n",
            "    Uninstalling statsmodels-0.14.5:\n",
            "      Successfully uninstalled statsmodels-0.14.5\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.11.0\n",
            "    Uninstalling gradio_client-1.11.0:\n",
            "      Successfully uninstalled gradio_client-1.11.0\n",
            "  Attempting uninstall: bokeh\n",
            "    Found existing installation: bokeh 3.7.3\n",
            "    Uninstalling bokeh-3.7.3:\n",
            "      Successfully uninstalled bokeh-3.7.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.1\n",
            "    Uninstalling transformers-4.54.1:\n",
            "      Successfully uninstalled transformers-4.54.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.2\n",
            "    Uninstalling seaborn-0.13.2:\n",
            "      Successfully uninstalled seaborn-0.13.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.9.0\n",
            "    Uninstalling accelerate-1.9.0:\n",
            "      Successfully uninstalled accelerate-1.9.0\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 4.1.0\n",
            "    Uninstalling sentence-transformers-4.1.0:\n",
            "      Successfully uninstalled sentence-transformers-4.1.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.39.0\n",
            "    Uninstalling gradio-5.39.0:\n",
            "      Successfully uninstalled gradio-5.39.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.1 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.1 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "tensorflow 2.19.0 requires tensorboard~=2.19.0, but you have tensorboard 2.16.2 which is incompatible.\n",
            "google-genai 1.28.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.28.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "panel 1.7.5 requires bokeh<3.8.0,>=3.5.0, but you have bokeh 3.3.4 which is incompatible.\n",
            "yfinance 0.2.65 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.1 which is incompatible.\n",
            "xarray 2025.7.1 requires pandas>=2.2, but you have pandas 2.1.1 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 aiofiles-23.2.1 blis-1.2.1 bokeh-3.3.4 datasets-2.19.1 evaluate-0.4.2 fsspec-2024.3.1 gradio-4.36.1 gradio-client-1.0.1 httpx-0.27.2 lightning-2.2.0.post0 lightning-utilities-0.15.2 markupsafe-2.1.5 matplotlib-3.8.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 ollama-0.2.1 packaging-24.2 pandas-2.1.1 pillow-10.4.0 pyarrow-hotfix-0.7 pytorch-lightning-2.5.2 scikit-learn-1.3.0 seaborn-0.12.2 sentence-transformers-3.0.1 statsmodels-0.14.0 tensorboard-2.16.2 thinc-8.3.4 tokenizers-0.19.1 tomlkit-0.12.0 torch-2.2.0 torchinfo-1.8.0 torchmetrics-1.8.1 tqdm-4.66.1 transformers-4.41.2 triton-2.2.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!test '{IN_COLAB}' = 'True' && wget  https://github.com/cam2149/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QTAGcSDXzeZd"
      },
      "outputs": [],
      "source": [
        "# Initialize SpaCy and VADER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "analyzer = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "try:\n",
        "    train_df = pd.read_csv(\"tweet_data/train.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: train.csv not found even after attempting download and extraction.\")\n",
        "    # You might want to add code here to handle the case where the file is still not found.\n",
        "\n",
        "\n",
        "# Data Exploration\n",
        "print(\"Dataset Preview:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nColumns:\", train_df.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaT6zxFu12sE",
        "outputId": "e2f44e16-0c58-4afc-8f07-f0d2117d1492"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Preview:\n",
            "       textID                                               text  \\\n",
            "0  cb774db0d1                I`d have responded, if I were going   \n",
            "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
            "2  088c60f138                          my boss is bullying me...   \n",
            "3  9642c003ef                     what interview! leave me alone   \n",
            "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
            "\n",
            "                         selected_text sentiment  \n",
            "0  I`d have responded, if I were going   neutral  \n",
            "1                             Sooo SAD  negative  \n",
            "2                          bullying me  negative  \n",
            "3                       leave me alone  negative  \n",
            "4                        Sons of ****,  negative  \n",
            "\n",
            "Columns: ['textID', 'text', 'selected_text', 'sentiment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of tokens in the processed_text column\n",
        "# Handle potential non-string values by converting them to strings and replacing NaN with empty strings\n",
        "token_count = train_df[\"text\"].astype(str).apply(lambda x: len(x.split())).sum()\n",
        "\n",
        "print(f\"\\nTotal number of tokens in the dataset: {token_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZytBFfd_8a73",
        "outputId": "ecf9a6b6-a2b4-459c-aeea-3f1537b8966a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total number of tokens in the dataset: 354572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of sentences in the text column\n",
        "# Handle potential non-string values by converting them to strings and replacing NaN with empty strings\n",
        "#sentence_count = train_df[\"text\"].astype(str).apply(lambda x: len(list(nlp(x).sents))).sum()\n",
        "\n",
        "#print(f\"\\nTotal number of sentences in the dataset: {sentence_count}\")"
      ],
      "metadata": {
        "id": "sX5EKGKq9PNW"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a row from the dataset (e.g., the first row)\n",
        "selected_row = train_df.iloc[0]\n",
        "text = selected_row[\"text\"]\n",
        "\n",
        "# Process the text with SpaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print information for each token\n",
        "print(f\"Analyzing text: '{text}'\\n\")\n",
        "print(\"{:20}{:20}{:20}{:20}\".format(\"Text\", \"POS\", \"dep\", \"lemma\"))\n",
        "for token in doc:\n",
        "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.dep_:{20}}{token.lemma_:{20}}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUDsZRIO-vf4",
        "outputId": "8d7248fc-bf7b-436f-abc9-b95ac4379eee"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing text: ' I`d have responded, if I were going'\n",
            "\n",
            "Text                POS                 dep                 lemma               \n",
            "                    SPACE               dep                                     \n",
            "I`d                 PROPN               nsubj               I`d                 \n",
            "have                AUX                 aux                 have                \n",
            "responded           VERB                ROOT                respond             \n",
            ",                   PUNCT               punct               ,                   \n",
            "if                  SCONJ               mark                if                  \n",
            "I                   PRON                nsubj               I                   \n",
            "were                AUX                 aux                 be                  \n",
            "going               VERB                advcl               go                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocessing function using SpaCy\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "train_df[\"processed_text\"] = train_df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "# Sentiment Analysis with VADER\n",
        "def get_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    compound = scores[\"compound\"]\n",
        "    if compound > 0.05:\n",
        "        return \"positive\"\n",
        "    elif compound < -0.05:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "train_df[\"predicted_sentiment\"] = train_df[\"processed_text\"].apply(get_sentiment)\n"
      ],
      "metadata": {
        "id": "KpaZCNBo8LdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation against provided labels\n",
        "print(\"\\nSentiment Prediction Evaluation:\")\n",
        "accuracy = accuracy_score(train_df[\"sentiment\"], train_df[\"predicted_sentiment\"])\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(classification_report(train_df[\"sentiment\"], train_df[\"predicted_sentiment\"]))\n",
        "\n",
        "# Text Justification Extraction with SpaCy Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "positive_pattern = [{\"LOWER\": {\"IN\": [\"good\", \"great\", \"excellent\", \"love\"]}}]\n",
        "negative_pattern = [{\"LOWER\": {\"IN\": [\"bad\", \"poor\", \"terrible\", \"hate\"]}}]\n",
        "matcher.add(\"PositiveWords\", [positive_pattern])\n",
        "matcher.add(\"NegativeWords\", [negative_pattern])\n",
        "\n",
        "def extract_justification(text):\n",
        "    if isinstance(text, str):  # Add this check\n",
        "        doc = nlp(text)\n",
        "        matches = matcher(doc)\n",
        "        if matches:\n",
        "            match_id, start, end = matches[0]\n",
        "            return doc[start:end].text\n",
        "    return \"\"\n",
        "\n",
        "train_df[\"extracted_text\"] = train_df[\"text\"].apply(extract_justification)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSample Results with Extracted Justification:\")\n",
        "print(train_df[[\"text\", \"sentiment\", \"predicted_sentiment\", \"selected_text\", \"extracted_text\"]].head())\n",
        "\n",
        "# Save results\n",
        "train_df.to_csv(\"tweet_sentiment_results.csv\", index=False)\n",
        "print(\"\\nResults saved to 'tweet_sentiment_results.csv'\")"
      ],
      "metadata": {
        "id": "CUlvgfyl8QLY"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "icesi-nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}