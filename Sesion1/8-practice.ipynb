{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8QtoXwvzeZQ"
      },
      "source": [
        "# NLP Basics Assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNG-UgJDzeZX"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Ohtar10/icesi-nlp/blob/main/Sesion1/8-practice.ipynb)\n",
        "\n",
        "En este notebook vamos a poner en práctica algunos de los conceptos vistos en los notebooks anteriores, aplicado a un corpus específico:\n",
        "[_An Occurrence at Owl Creek Bridge_](https://en.wikipedia.org/wiki/An_Occurrence_at_Owl_Creek_Bridge) por Ambrose Bierce (1890). Esta historia es de dominio público y el corpus fue obtenido de [Project Gutenberg](https://www.gutenberg.org/ebooks/375.txt.utf-8).\n",
        "\n",
        "## Referencias\n",
        "* [NLP - Natural Language Processing With Python](https://www.udemy.com/course/nlp-natural-language-processing-with-python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_TYq6mbzeZZ",
        "outputId": "796ac2dc-01d2-4f26-de02-4c09e52dac08"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_1606223/2396000874.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  import pkg_resources\n"
          ]
        }
      ],
      "source": [
        "import pkg_resources\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "installed_packages = [package.key for package in pkg_resources.working_set]\n",
        "IN_COLAB = 'google-colab' in installed_packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSOLJWUzzeZd"
      },
      "outputs": [],
      "source": [
        "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/requirements.txt && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTAGcSDXzeZd"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL to perform standard imports:\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grC8CNUjzeZe"
      },
      "source": [
        "**1. Creamos el documento desde el archivo `owlcreek.txt`**<br>\n",
        "> Pista: Usa `with open('./owlcreek.txt') as f:`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWphvnQrzeZf"
      },
      "outputs": [],
      "source": [
        "!test '{IN_COLAB}' = 'True' && wget  https://github.com/Ohtar10/icesi-nlp/raw/refs/heads/main/Sesion1/owlcreek.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGBXlIadzeZf"
      },
      "outputs": [],
      "source": [
        "with open('./owlcreek.txt') as file:\n",
        "    doc = nlp(file.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5dIl3QXzeZg",
        "outputId": "5173d624-ce5d-4fce-bef2-9bf3c90675c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AN OCCURRENCE AT OWL CREEK BRIDGE\n",
              "\n",
              "by Ambrose Bierce\n",
              "\n",
              "I\n",
              "\n",
              "A man stood upon a railroad bridge in northern Alabama, looking down\n",
              "into the swift water twenty feet below.  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "doc[:36]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEMLgO8HzeZg"
      },
      "source": [
        "El documento fue cargado exitosamente!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8hSsCwezeZh"
      },
      "source": [
        "**2. Cuantos tokens hay en el archivo?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4OlUk1tzeZh",
        "outputId": "32ce859c-1a2e-4150-881f-bfe6fa5446db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4835"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1riqkVxzeZi"
      },
      "source": [
        "**3. Cuantas oraciones hay en el archivo?**\n",
        "<br>Pista: Necesitarás una lista primero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shP95u-uzeZi",
        "outputId": "7d350161-0cb0-4c6e-c03f-2e3ff81e6c1b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "204"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences = list(doc.sents)\n",
        "len(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CqCsol1zeZj"
      },
      "source": [
        "**4. Imprime la segunda oración del documento**\n",
        "<br> Pista: Los índices comienzan en 0 y el título cuenta como la primera oración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abyMnpVfzeZj",
        "outputId": "ed9e81a0-e336-4aa5-8b3e-ca4c11827a12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "The man's hands were behind\n",
              "his back, the wrists bound with a cord.  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentences[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fklVEbEtzeZj"
      },
      "source": [
        "**5. Por cada token en la oración anterior, imprime su `text`, `POS` tag, `dep` tag y `lemma`**\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFSgEEOazeZj",
        "outputId": "9e31989d-3bcf-463e-8289-a9d23362f6ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text                POS                 dep                 lemma               \n",
            "The                 DET                 det                 the                 \n",
            "man                 NOUN                poss                man                 \n",
            "'s                  PART                case                's                  \n",
            "hands               NOUN                nsubj               hand                \n",
            "were                AUX                 ROOT                be                  \n",
            "behind              ADP                 prep                behind              \n",
            "\n",
            "                   SPACE               dep                 \n",
            "                   \n",
            "his                 PRON                poss                his                 \n",
            "back                NOUN                pobj                back                \n",
            ",                   PUNCT               punct               ,                   \n",
            "the                 DET                 det                 the                 \n",
            "wrists              NOUN                appos               wrist               \n",
            "bound               VERB                acl                 bind                \n",
            "with                ADP                 prep                with                \n",
            "a                   DET                 det                 a                   \n",
            "cord                NOUN                pobj                cord                \n",
            ".                   PUNCT               punct               .                   \n",
            "                    SPACE               dep                                     \n"
          ]
        }
      ],
      "source": [
        "print(\"{:20}{:20}{:20}{:20}\".format(\"Text\", \"POS\", \"dep\", \"lemma\"))\n",
        "for token in sentences[1]:\n",
        "    print(f\"{token.text:{20}}{token.pos_:{20}}{token.dep_:{20}}{token.lemma_:{20}}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iJDEWeYzeZk"
      },
      "source": [
        "**6. Implementa un matcher llamado *Swimming* que encuentre las ocurrencias de la frase *swimming vigorously* Write a matcher called 'Swimming' that finds**\n",
        "<br>\n",
        "Pista: Deberías incluir un patrón`'IS_SPACE': True` entre las dos palabras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JORmjJkAzeZk"
      },
      "outputs": [],
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "pattern = [{'LOWER': 'swimming'}, {'IS_SPACE': True}, {'LOWER': 'vigorously'}]\n",
        "matcher.add(\"Swimming\", [pattern])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc6C9xmMzeZk",
        "outputId": "d836fd7f-741e-4b3a-9f27-ec0a0b803592"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(12881893835109366681, 1274, 1277), (12881893835109366681, 3609, 3612)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "found_matches = matcher(doc)\n",
        "found_matches\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIWcBe0mzeZl"
      },
      "source": [
        "**7. Imprime el texto al rededor de cada match encontrado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVEIY0xizeZl",
        "outputId": "9146c797-e25b-45a2-a00a-563153d6ad58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "By diving I could evade the bullets and, swimming\n",
              "vigorously, reach the bank, take to the woods and get away home"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start, end = found_matches[0][1:]\n",
        "doc[start-9:end+13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfAKSKTNzeZl",
        "outputId": "9db26522-d43e-47ac-8577-52b40a9a0237"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "over his shoulder; he was now swimming\n",
              "vigorously with the current.  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "start, end = found_matches[1][1:]\n",
        "doc[start-7:end+5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OAQA7XOzeZm"
      },
      "source": [
        "**8. Imprime la oración que contiene cada match encontrado**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BPV7Fu5zeZm",
        "outputId": "88f3cae7-8d24-4649-fffe-13edf483d890"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "By diving I could evade the bullets and, swimming\n",
            "vigorously, reach the bank, take to the woods and get away home.   \n",
            "\n",
            "The hunted man saw all this over his shoulder; he was now swimming\n",
            "vigorously with the current.   \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sentence in sentences:\n",
        "    for _, start, end in found_matches:\n",
        "        if sentence.start <= start and sentence.end >= end:\n",
        "            print(sentence.text, '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82b811fd",
        "outputId": "8e22af46-21db-4746-ed75-f4f809a359cf"
      },
      "source": [],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.8.3)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!pip install vaderSentiment\n",
        "\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from spacy.matcher import Matcher\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Initialize SpaCy and VADER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Download and load the Kaggle dataset\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle competitions download -c tweet-sentiment-extraction\n",
        "!unzip -o tweet-sentiment-extraction.zip -d tweet_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3sldvIuzhOo",
        "outputId": "0a8d31a2-011f-4878-a98e-0fbec7ea4e5e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.8.3)\n",
            "Downloading tweet-sentiment-extraction.zip to /content\n",
            "  0% 0.00/1.39M [00:00<?, ?B/s]\n",
            "100% 1.39M/1.39M [00:00<00:00, 648MB/s]\n",
            "Archive:  tweet-sentiment-extraction.zip\n",
            "  inflating: tweet_data/sample_submission.csv  \n",
            "  inflating: tweet_data/test.csv     \n",
            "  inflating: tweet_data/train.csv    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from spacy.matcher import Matcher\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import os\n",
        "\n",
        "# Initialize SpaCy and VADER\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Download and load the Kaggle dataset\n",
        "os.system(\"kaggle competitions download -c tweet-sentiment-extraction -p tweet_data --unzip\")\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    train_df = pd.read_csv(\"tweet_data/train.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: train.csv not found even after attempting download and extraction.\")\n",
        "    # You might want to add code here to handle the case where the file is still not found.\n",
        "\n",
        "\n",
        "# Data Exploration\n",
        "print(\"Dataset Preview:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nColumns:\", train_df.columns.tolist())\n",
        "\n",
        "# Preprocessing function using SpaCy\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    doc = nlp(str(text))\n",
        "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "train_df[\"processed_text\"] = train_df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "# Sentiment Analysis with VADER\n",
        "def get_sentiment(text):\n",
        "    scores = analyzer.polarity_scores(text)\n",
        "    compound = scores[\"compound\"]\n",
        "    if compound > 0.05:\n",
        "        return \"positive\"\n",
        "    elif compound < -0.05:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "train_df[\"predicted_sentiment\"] = train_df[\"processed_text\"].apply(get_sentiment)\n",
        "\n",
        "# Evaluation against provided labels\n",
        "print(\"\\nSentiment Prediction Evaluation:\")\n",
        "accuracy = accuracy_score(train_df[\"sentiment\"], train_df[\"predicted_sentiment\"])\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(classification_report(train_df[\"sentiment\"], train_df[\"predicted_sentiment\"]))\n",
        "\n",
        "# Text Justification Extraction with SpaCy Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "positive_pattern = [{\"LOWER\": {\"IN\": [\"good\", \"great\", \"excellent\", \"love\"]}}]\n",
        "negative_pattern = [{\"LOWER\": {\"IN\": [\"bad\", \"poor\", \"terrible\", \"hate\"]}}]\n",
        "matcher.add(\"PositiveWords\", [positive_pattern])\n",
        "matcher.add(\"NegativeWords\", [negative_pattern])\n",
        "\n",
        "def extract_justification(text):\n",
        "    if isinstance(text, str):  # Add this check\n",
        "        doc = nlp(text)\n",
        "        matches = matcher(doc)\n",
        "        if matches:\n",
        "            match_id, start, end = matches[0]\n",
        "            return doc[start:end].text\n",
        "    return \"\"\n",
        "\n",
        "train_df[\"extracted_text\"] = train_df[\"text\"].apply(extract_justification)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nSample Results with Extracted Justification:\")\n",
        "print(train_df[[\"text\", \"sentiment\", \"predicted_sentiment\", \"selected_text\", \"extracted_text\"]].head())\n",
        "\n",
        "# Save results\n",
        "train_df.to_csv(\"tweet_sentiment_results.csv\", index=False)\n",
        "print(\"\\nResults saved to 'tweet_sentiment_results.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "DaT6zxFu12sE",
        "outputId": "2dd93fa6-e0f1-4346-905d-5743b6d7d02c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'vaderSentiment'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-337862018.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatcher\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMatcher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'vaderSentiment'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "icesi-nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
